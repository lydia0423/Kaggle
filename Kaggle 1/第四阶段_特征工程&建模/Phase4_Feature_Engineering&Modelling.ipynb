{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程&数据建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>Tip: </b>\n",
    "    \n",
    "- 完成【第四阶段】之前，请先run前面所有的代码！\n",
    "- 图片若显示不出，需要连一下vpn，图片存储在github中了。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<center><b>【第四阶段】项目日志（2022年3月7日）</b></center>\n",
    "    \n",
    "   \n",
    "**第四节阶段核心目的：**基于上阶段处理过的数据`train_balanced_update`和`final_meta`进行特征工程（Feature Engineering）并且根据树形算法筛选特征，并代入XGBosst进行概率预测。\n",
    "   \n",
    "    \n",
    "**难度（最高5星）：**⭐⭐⭐\n",
    "    \n",
    "**第三阶段周期：** 2022年3月1日 至 2022年3月9日（北京时间）\n",
    "\n",
    "\n",
    "Good Luck!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小伙伴们，终于进入到鲸析数据科学项目第一期的最终阶段了，坚持就是胜利！\n",
    "\n",
    "前三个阶段，我们关于train进行的元数据处理、分析并且对于不平衡类别数据进行了转换，也基于元数据，对不同的变量类型的特征加以深入的数据分析，并且学习了类别型变量的编码方法。\n",
    "\n",
    "那么这里，我们要对train和test同时做编码处理，在特征工程的过程中，我们要保证train和test的维度是保持一致的，所以特征工程是可以同时对train和test做的，下面我们来一步一步看一下！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train_df.csv',index_col=0,dtype={'id': np.int32, 'target': np.int8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/test_df.csv',index_col=0,dtype={'id': np.int32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('target',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = pd.concat([train,test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_meta = pd.read_csv('./data/final_meta.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据之前保存的final_meta找出所有保存的连续型变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_meta[(final_meta.变量类型 == 'interval')|(final_meta.变量类型 == 'ratio') & (final_meta.是否保留 == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = final_meta[(final_meta.变量类型 == 'interval')|(final_meta.变量类型 == 'ratio') & (final_meta.是否保留 == True)].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 计算各特征的相关系数矩阵（对称）\n",
    "corr_mat = full_set[continuous_cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heatmap热力图正好作用于相关系数矩阵，可以帮我们更直观地观测总结各连续型特征之间的相关性强弱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [20,10])   # 设置画布大小\n",
    "sns.heatmap(data = corr_mat,    # \n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            square=True,\n",
    "           annot = True,  # 显示文字\n",
    "          fmt='.2f',      # 保留两位\n",
    "           cmap = 'Blues',   # 颜色\n",
    "           linewidths = .3,  # 分割线宽度\n",
    "           cbar_kws={\"shrink\": .75})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "颜色越深表示2者相关性越强！注意：忽略掉对角线全是1的这一部分，因为，每个特征和自身的相关系数是1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，下面这些连续型变量的相关程度较高：\n",
    "\n",
    "- ps_reg_02 和 ps_reg_03 (0.75)\n",
    "- ps_car_13 和 ps_car_12 ps_car_14 ps_car_15 都很相关！\n",
    "- ps_car_11_cat_tar_enc 和 ps_car_12 ps_car_13 ps_car_14 ps_car_15相关度也不低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 1</b>: \n",
    "    \n",
    "所以现在该怎么办呢？有些变量之间是存在较强的相关性的，是需要对这种情况如何处理呢？\n",
    "\n",
    "大家有什么好思路么？（2分）\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "鉴于变量之间的较强相关性，我不能把他们都带入模型中，因为相关特征不能提升模型表现，对于：\n",
    "\n",
    "1. 线性模型\n",
    "\n",
    "会出现多重共线性的问题，从而导致解析解的波动较大，方差较大，不稳定。\n",
    "\n",
    "2. 树形模型\n",
    "\n",
    "树形模型基于各特征的信息增益对其分割，适合处理不同特征之间的交互性，但是高相关性会掩盖其中的【交互性】。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 2</b>: \n",
    "    \n",
    "查查资料，简述特征的相关性和交互性的区别是什么？（2分）\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature correlation\n",
    "\n",
    "特征的相关性是说，不考虑target，两个特征之间是互相依赖的，或者说，如果我增大特征A的值，特征B的值也会发生线性的变化（变大或者变小）。\n",
    "\n",
    "- Feature Interaction\n",
    "\n",
    "特征交互性是说，现在有两个不相关的特征，比如：房子大小（特征A）和房子的位置（特征B），他们并不相关，我们的target是房价（要去预测的），那么这俩特征交互起来，或者说以某种方式合并起来，是不是会有`1+1>2`的效果，比如基本的【加减乘除】的组合。\n",
    "\n",
    "所以总结起来：\n",
    "\n",
    "\n",
    "|          |                    interaction 交互性                    |                      correlation 相关性                      |\n",
    "| :------: | :------------------------------------------------------: | :----------------------------------------------------------: |\n",
    "| 研究目的 |   判断是否有特征组合的可能（特征组合、加强）<br />降维   |   判断是否有强相关特征，使模型过拟合（特征筛选）<br />降维   |\n",
    "|   难点   | 不好判断交互度，构造方法多样，比如：经验驱动、数据驱动。 | 相关性虽然易于判断，但是强相关特征如何筛选，如何降维也比较困难。 |\n",
    "| 作用变量 |                          连续型                          |                            连续型                            |\n",
    "| 分析手段 |                           SHAP                           |                    VarianceThreshold/PCA                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_balanced_update['ps_reg_03'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下分布！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in continuous_cols:\n",
    "    sns.displot(full_set[col])\n",
    "    plt.xticks(ticks = np.linspace(start = full_set[col].min(),\n",
    "                                  stop = full_set[col].max(),\n",
    "                                  num = 10),rotation = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉有一些变量虽然是连续型的，但是并不是会形成传统意义上的概率分布，还会有离散型数据的性质出现，并且有很多尖峰的情况，普遍对应着肥尾。\n",
    "\n",
    "所以我们尝试采用特征离散化，来强化突出特征的关键性信息（主要集中在尖峰附近），并简化特征，避免过拟合。\n",
    "\n",
    "同时避免丢失离群值（离群值无法定义为异常值，并不是分布的异常就说明其不合理性，在此没有更多的evidence证明，所以不剔除离群值）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征分箱\n",
    "\n",
    "\n",
    "#### 分箱算法简介\n",
    "\n",
    "先来讲讲什么是分箱算法，根据字面意思就是\n",
    "\n",
    "把数据按照不同的【规则】分到不同的【箱子】里。\n",
    "\n",
    "其实分箱是特征工程的一种，可以理解为一种**连续数据**变为**离散数据**的建模方式。\n",
    "\n",
    " \n",
    "\n",
    "> 举例：比如有一组连续数据，比如为`25，14，68，43，63`。假设数据的分箱逻辑是大于50为0，小于50为1，那么最终数据会变成1，1，0，1，0，数据就离散化了。当然分箱有很多方式，大致作用就是把连续数据按照一定的规则离散化。\n",
    "\n",
    " \n",
    "\n",
    "#### 分箱算法的好处\n",
    " \n",
    "其优点等同于数据离散化的好处：\n",
    "\n",
    "- 减少过拟合的风险，因为分箱相当于对于数据去粗粒度描述\n",
    "\n",
    "- 增加稀疏数据的概率，减少计算量，因为0的数据变多了\n",
    "\n",
    "- 减少噪声数据的影响，比如一组数据按照0~100均匀分布，当数据中突然出现一个10000的数据，如果不做分箱的化会对Logistic Regression这种模型的训练造成很大影响\n",
    "\n",
    "- 方便特征衍生，因为数据离散化后就可以把特征直接相互做内积提升特征维度\n",
    "\n",
    "- 离散化后可以提升模型的鲁棒性，比如我们有一组数据是年龄，比如A30岁、B50岁，到了第二年A变成31岁，B变成51岁，所有数据都变了理论上要更新模型。但是如果数据分箱了之后，比如分箱逻辑是小于40岁为0，大于40岁为1，则第一年和第二年数据没有变化，模型也不用变\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 3</b>: \n",
    "    \n",
    "- 根据上图，请研究思考哪些连续型变量需要尝试进行【特征分箱】的操作？\n",
    "- 请叙述 pd.qcut 和 pd.cut 的区别，我们在变量离散化时用哪个更好？\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/datoujinggzj/Whale_Project/blob/main/porto-seguro-safe-driver-prediction/pic/phase4/woe_computation.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_iv_encoding(data, feat, target, max_intervals, verbose = False):\n",
    "    \n",
    "    feat_bins = pd.qcut(x = data[feat], q = max_intervals, duplicates='drop')\n",
    "    gi = pd.crosstab(feat_bins,data[target])\n",
    "    gb = pd.Series(data=data[target]).value_counts()\n",
    "\n",
    "    bad = gi[1]/gi[0]\n",
    "    good = gb[1]/gb[0]\n",
    "\n",
    "    # 计算woe\n",
    "    woe = np.log(bad) - np.log(good)\n",
    "\n",
    "    # 计算iv\n",
    "    iv = (bad-good)*woe\n",
    "\n",
    "    # 计算整个特征的iv\n",
    "    f_iv = iv.sum()  # 5.2958917587905745\n",
    "    if verbose == True:\n",
    "        print(f\"根据当前的间隔数{max_intervals}，特征{feat}所计算的总information value为：{f_iv}\")\n",
    "        print('='*80)\n",
    "\n",
    "    # 进行映射操作\n",
    "    dic = iv.to_dict()\n",
    "\n",
    "    iv_bins = feat_bins.map(dic)  # 连续型变量离散化\n",
    "\n",
    "    return iv_bins.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/datoujinggzj/Whale_Project/blob/main/porto-seguro-safe-driver-prediction/pic/phase4/woe_iv.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_col_bin = ['ps_reg_01','ps_car_12','ps_car_13','ps_car_14']    # 填写\n",
    "\n",
    "\n",
    "for col in continuous_col_bin:\n",
    "    full_set[f\"{col}_bin_woe\"] = woe_iv_encoding(data = full_set, feat = col, target = 'target', max_intervals = 20)\n",
    "    sns.displot(full_set[f\"{col}_bin_woe\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 4</b>: \n",
    "    \n",
    "- 简述【特征交互】的意义和原理\n",
    "- 请叙述 PolynomialFeatures 的 degree interaction_only include_bias的含义\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "interactions = pd.DataFrame(data=poly.fit_transform(full_set[continuous_cols]), \n",
    "                            columns=poly.get_feature_names_out(continuous_cols))\n",
    "\n",
    "\n",
    "interactions.drop(continuous_cols, axis=1, inplace=True)  # Remove the original columns\n",
    "# Concat the interaction variables to the train data\n",
    "print('特征交互前，训练集有 {}个变量 '.format(full_set.shape[1]))\n",
    "full_set = pd.concat([full_set, interactions], axis=1)\n",
    "print('特征交互后，训练集有 {}个变量'.format(full_set.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_cols = final_meta[(final_meta.变量类型 == 'nominal') & (final_meta.是否保留 == True)].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 5</b>: \n",
    "    \n",
    "- 为什么要去除variance较低的特征？\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=.01)\n",
    "selector.fit(full_set.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n",
    "\n",
    "f = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n",
    "\n",
    "v = full_set.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\n",
    "print('{} variables have too low variance.'.format(len(v)))\n",
    "print('These variables are {}'.format(list(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 6</b>: \n",
    "    \n",
    "- 'weight', 'gain', 'cover' 这三种衡量特征重要度的strategy区别是什么？\n",
    "- 了解树形算法的基本概念:\n",
    "    - https://statquest.org/xgboost-part-1-xgboost-trees-for-regression/\n",
    "    - https://statquest.org/xgboost-part-2-xgboost-trees-for-classification/\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = [100,20])\n",
    "\n",
    "X = full_set.loc[train.index].drop(['id', 'target'], axis=1)\n",
    "y = full_set.loc[train.index].target\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(X, y)\n",
    "# plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define subplot grid\n",
    "fig, axs = plt.subplots(nrows=5, ncols=1, figsize=(15, 80))\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.1)\n",
    "\n",
    "types = ['weight', 'gain', 'cover', 'total_gain', 'total_cover']\n",
    "# loop through tickers and axes\n",
    "colors = ['#ff7f01','#08aebd','#fc5531','#139948','#8950fe']\n",
    "for ty, ax, color in zip(types, axs.ravel(), colors):\n",
    "    # filter df for ticker and plot on specified axes\n",
    "    plot_importance(ax = ax, booster = model,importance_type=ty, color = color)\n",
    "\n",
    "    # chart formatting\n",
    "    ax.set_title(ty.upper(),fontsize = 22)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 【gain】增益意味着相应特征对模型的相对贡献，通过获取模型中每棵树的每个特征的贡献来计算。与另一个特征相比，该指标的值越高意味着它对于生成预测更重要。\n",
    "- 【Coverage】是指与该特征相关的观察的相对数量。例如，如果您有 100 个观测值，4 个特征和 3 棵树，假设在 tree1、tree2 和 tree3 中分别使用 feature1 来决定 10、5 和 2 个观测值的叶子节点；那么该指标将将此特征的覆盖率计为 10+5+2 = 17 个观察值。这将针对所有 4 个功能进行计算，并且覆盖率将 17 表示为所有功能的覆盖率指标的百分比。\n",
    "- 【weights】是表示特定特征在模型树中出现的相对次数的百分比。在上面的例子中，如果feature1出现2次分裂，则tree1、tree2和tree3各有1次分裂和3次分裂；那么特征 1 的权重将为 2+1+3 = 6。特征 1 的频率计算为其权重占所有特征权重的百​​分比。\n",
    "\n",
    "\n",
    "> 增益是解释每个特征的相对重要性最相关的属性。\n",
    "“增益”是特征为其所在分支带来的准确性提高。这个想法是，在将特征 X 上的新拆分添加到分支之前，有一些错误分类的元素，在此特征上添加拆分后，有两个新分支，每个分支都更准确（一个分支说如果你的观察是在这个分支上，那么它应该被归类为 1，而另一个分支则完全相反）。\n",
    "“覆盖率”衡量一个特征所涉及的观察的相对数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果再针对这些数据进行降维，避免过拟合呢，我们尝试一下PCA吧！\n",
    "\n",
    "- https://www.youtube.com/watch?v=HMOI_lkzW08&ab_channel=StatQuestwithJoshStarmer 【初识pca】\n",
    "\n",
    "- https://www.youtube.com/watch?v=Lsue2gEM9D0&ab_channel=StatQuestwithJoshStarmer 【pca in python】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "print('\\nPCA执行中...')\n",
    "pca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print('Total Explained variance: %.4f' % pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12,8])\n",
    "\n",
    "pd.Series(pca.explained_variance_ratio_).cumsum().plot()\n",
    "\n",
    "plt.plot(range(n_comp),[0.98]*20, 'r--')\n",
    "plt.xticks(ticks = range(n_comp))\n",
    "plt.yticks(ticks = np.linspace(0.9,1,11))\n",
    "plt.text(12,0.97,'cumulative explained var: \\n0.98',fontsize = 12, color = '#20beff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1],c=y,cmap='viridis',alpha = .7)\n",
    "plt.xlabel('pc1')\n",
    "plt.ylabel('pc2')\n",
    "plt.title(\n",
    "        \"第一第二主成分散点分布图\")\n",
    "plt.xlabel(\"第一主成分解释 %.1f %% 方差\" % (\n",
    "        pca.explained_variance_ratio_[0] * 100.0))\n",
    "plt.ylabel(\"第二主成分解释 %.1f %% 方差\" % (\n",
    "        pca.explained_variance_ratio_[1] * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程的strategy作为key，对应的变量名组成的list作为value\n",
    "feat_dict = {}\n",
    "for thres in ['median','mean','1.25*mean']:\n",
    "    model_select = SelectFromModel(model, threshold=thres, prefit=True)\n",
    "    print(f'筛选前总计：{X.shape[1]}个特征')\n",
    "    n_features = model_select.transform(X.values).shape[1]\n",
    "    print(f'筛选后总计： {n_features}个特征【{thres}】')\n",
    "    print('#'*60)\n",
    "    selected_vars = list(X.columns[model_select.get_support()])\n",
    "    feat_dict[thres] = selected_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ty in types:\n",
    "    feat_dict[ty] = list(model.get_booster().get_score(importance_type=ty).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = full_set.loc[train.index][feat_dict['mean']+['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = full_set.loc[test.index][feat_dict['mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于不平衡数据，我们不能用传统的accuracy来衡量模型的好坏，本项目给出了一个归一基尼系数，作为本项目的唯一指标。\n",
    "\n",
    "\n",
    "参考：\n",
    "- https://www.kaggle.com/batzner/gini-coefficient-an-intuitive-explanation\n",
    "- https://stats.stackexchange.com/questions/306287/why-use-normalized-gini-score-instead-of-auc-as-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证：\n",
    "https://www.youtube.com/watch?v=fSytzGwwBVw&ab_channel=StatQuestwithJoshStarmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数调节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的参数有很多，如何选择合适的参数，是个很头疼的问题，所以建模的玄学就在这里，一些超参数必须主动调节，可以固定筛选范围，一个一个试，看哪个表现更好，但是这么做成本巨大，下面这一堆代码，运行完，大概要16min。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         'min_child_weight': [1, 5, 10],\n",
    "#         'gamma': [0.5, 1, 1.5, 2, 5, 10],\n",
    "#         'subsample': [0.6, 0.8, 1.0],\n",
    "#         'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#         'max_depth': [3, 4, 5]\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = XGBClassifier(learning_rate=0.06, n_estimators=300, objective='binary:logistic',nthread=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# def timer(start_time=None):\n",
    "#     if not start_time:\n",
    "#         start_time = datetime.now()\n",
    "#         return start_time\n",
    "#     elif start_time:\n",
    "#         thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "#         tmin, tsec = divmod(temp_sec, 60)\n",
    "#         print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "        \n",
    "        \n",
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# folds = 3\n",
    "# param_comb = 5\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n",
    "\n",
    "# # Here we go\n",
    "# start_time = timer(None)\n",
    "# random_search.fit(X, y)\n",
    "# timer(start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n All results:')\n",
    "# print(random_search.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(random_search.best_estimator_)\n",
    "# print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "# print(random_search.best_score_ * 2 - 1)\n",
    "# print('\\n Best hyperparameters:')\n",
    "# print(random_search.best_params_)\n",
    "# results = pd.DataFrame(random_search.cv_results_)\n",
    "# results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调参之后，较优的参数组合\n",
    "MAX_ROUNDS = 400\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "\n",
    "model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=4,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        booster = 'gbtree',\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=.8,\n",
    "                        min_child_weight=6,\n",
    "                        colsample_bytree=.8,\n",
    "                        scale_pos_weight=1.6,\n",
    "                        gamma=10,\n",
    "                        reg_alpha=8,\n",
    "                        reg_lambda=1.3,\n",
    "                        nthread=4,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_gini(df_train,tar_enc = True):\n",
    "    \n",
    "    '''\n",
    "    df_train: processed train data\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    y = df_train.target\n",
    "    X = df_train.drop('target',axis=1)\n",
    "    \n",
    "    \n",
    "    y_valid_pred = 0*y\n",
    "    y_test_pred = 0\n",
    "    \n",
    "    MAX_ROUNDS = 400\n",
    "    OPTIMIZE_ROUNDS = False\n",
    "    LEARNING_RATE = 0.07\n",
    "    EARLY_STOPPING_ROUNDS = 50  \n",
    "    \n",
    "\n",
    "    from target_encoding import target_encode\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(df_train)):\n",
    "\n",
    "        # 分成训练集、验证集、测试集\n",
    "        y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "        X_test = final_test.copy()\n",
    "        print( f\"\\n{i}折交叉验证： \")\n",
    "        \n",
    "        if tar_enc == True:\n",
    "            f_cat = [f for f in X.columns if '_cat' in f and 'tar_enc' not in  f]\n",
    "            for f in f_cat:\n",
    "                X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                                trn_series=X_train[f],\n",
    "                                                                val_series=X_valid[f],\n",
    "                                                                tst_series=X_test[f],\n",
    "                                                                target=y_train,\n",
    "                                                                min_samples_leaf=100,\n",
    "                                                                smoothing=10,\n",
    "                                                                noise_level=0\n",
    "                                                                )\n",
    "\n",
    "    #     from category_encoders.target_encoder import TargetEncoder\n",
    "    #     tar_enc = TargetEncoder(cols = f_cat).fit(X_train,y_train)\n",
    "    #     X_train = tar_enc.transform(X_train) # 转换训练集\n",
    "    #     X_test = tar_enc.transform(X_test) # 转换测试集\n",
    "\n",
    "\n",
    "            X_train.drop(f_cat,axis=1,inplace=True)\n",
    "            X_valid.drop(f_cat,axis=1,inplace=True)\n",
    "            X_test.drop(f_cat,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "        # 对于当前折，跑XGB\n",
    "        if OPTIMIZE_ROUNDS:\n",
    "            eval_set=[(X_valid,y_valid)]\n",
    "            fit_model = model.fit( X_train, y_train, \n",
    "                                   eval_set=eval_set,\n",
    "                                   eval_metric=gini_xgb,\n",
    "                                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                                   verbose=False\n",
    "                                 )\n",
    "            print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "            print( \"  Best gini = \", model.best_score )\n",
    "        else:\n",
    "            fit_model = model.fit( X_train, y_train )\n",
    "\n",
    "        # 生成验证集的预测结果\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "        print( \"  normalized gini coefficent = \", eval_gini(y_valid, pred) )\n",
    "        y_valid_pred.iloc[test_index] = pred\n",
    "\n",
    "        # 累积计算测试集预测结果\n",
    "        y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        del X_test, X_train, X_valid, y_train\n",
    "\n",
    "    y_test_pred /= K  # 取各fold结果均值\n",
    "\n",
    "    print( \"\\n整个训练集（合并）的normalized gini coefficent:\" )\n",
    "    print( \"  final normalized gini coefficent = \", eval_gini(y, y_valid_pred) )\n",
    "    \n",
    "    return y_test_pred,eval_gini(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred, gini_score = XGB_gini(df_train=final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = final_test.index.values\n",
    "submission['target'] = y_test_pred\n",
    "submission.to_csv('xgb_submit.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![H65i3d.gif](https://s4.ax1x.com/2022/02/14/H65i3d.gif)](https://imgtu.com/i/H65i3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de7a1f63-ccb4-49f0-bf3e-8e1181c5c5be",
    "_uuid": "b8aa7aee1c138ab00cecc0197b33d07d9a1cc1e0"
   },
   "source": [
    "<img src=\"https://s4.ax1x.com/2022/02/14/H65vxs.jpg\" alt=\"image-20220214212808413\" style=\"zoom:25%;\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
